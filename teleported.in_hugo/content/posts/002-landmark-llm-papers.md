+++
date        = "2025-12-20T23:27:27-04:00"
title       = "Landmark LLM Papers"
description = ""
tags        = [ "llm", "papers"]
categories  = [ "papers" ]
slug        = "landmark-llm-papers"
draft       = false
+++

### Introduction

A list of curated landmark papers in the field of LLMs.

### Foundational

- [Efficient Estimation of Word Representations in Vector Space (Word2Vec) (2013)](https://arxiv.org/abs/1301.3781)
- [GloVe: Global Vectors for Word Representation (2014)](https://nlp.stanford.edu/pubs/glove.pdf)
- [Neural Machine Translation by Jointly Learning to Align and Translate (2014)](https://arxiv.org/abs/1409.0473)
    - Introduced the concept of attention

### Transformer

- [Attention Is All You Need (2017)](https://arxiv.org/abs/1706.03762)
    - Introduced the Transformer architecture
- [Self-Attention with Relative Position Representations (2018)](https://arxiv.org/abs/1803.02155)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)](https://arxiv.org/abs/1810.04805)
- [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation (2021)](https://arxiv.org/abs/2104.09813)
- [RoFormer: Enhanced Transformer with Rotary Position Embedding (2021)](https://arxiv.org/abs/2104.09864)


### Large Language Models

- [Universal Language Model Fine-tuning for Text Classification (ULMFiT)(2018)](https://arxiv.org/abs/1801.06146)
- [Improving Language Understanding by Generative Pre-Training (GPT-1)(2018)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- [Language Models are Unsupervised Multitask Learners (GPT-2)(2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [Language Models are Few-Shot Learners (GPT-3) (2020)](https://arxiv.org/abs/2005.14165)
- [What Can Transformers Learn In-Context? A Case Study of Simple Function Classes (2021)](https://arxiv.org/abs/2101.03961)
- [GPT-4 Technical Report (2023)](https://arxiv.org/abs/2303.08774)

### Alignment

- [Deep reinforcement learning from human preferences (2017)](https://arxiv.org/abs/1706.03741)
- [Training language models to follow instructions with human feedback (2022)](https://arxiv.org/abs/2203.02155)
- [Constitutional AI: Harmlessness from AI Feedback (2022)](https://arxiv.org/abs/2203.02155)

### Scaling Laws, Emergence

- [Scaling Laws for Neural Language Models (2020)](https://arxiv.org/abs/2001.08361)
- [Training Compute-Optimal Large Language Models (2022)](https://arxiv.org/abs/2203.05556)
- [Emergent Abilities of Large Language Models (2022)](https://arxiv.org/abs/2206.07682)

### Prompt / Context Engineering

- [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (2021)](https://arxiv.org/abs/2109.08203)


### Efficient Transformers

- [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (2019)](https://arxiv.org/abs/1901.02860)
- [Reformer: The Efficient Transformer (2019)](https://arxiv.org/abs/1904.00969)
- [Longformer: The Long-Document Transformer (2020)](https://arxiv.org/abs/2004.05150)
- [Generating Long Sequences with Sparse Transformers (2020)](https://arxiv.org/abs/2004.05718)
- [Big Bird: Transformers for Longer Sequences (2020)](https://arxiv.org/abs/2007.04509)
- [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (2022)](https://arxiv.org/abs/2205.14135)

### Survey Papers

- [A Survey of Transformers (2022)](https://arxiv.org/abs/2106.04554)
- [Efficient Transformers: A Survey (2020)](https://arxiv.org/abs/2009.06732)
- [A Survey of Large Language Models (2023)](https://arxiv.org/abs/2303.18223)
- [On the Opportunities and Risks of Foundation Models (2022)](https://arxiv.org/abs/2108.07258)
- [Pre-train, Prompt, and Predict: A Survey of Prompting Methods in NLP (2021)](https://arxiv.org/abs/2107.13586)
- [Speed Always Wins: A Survey on Efficient Architectures for Large Language Models (2025)](https://arxiv.org/abs/2508.09834)

