<!doctype html><html lang=en dir=auto data-theme=light><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Papers | Teleported.in</title><meta name=keywords content><meta name=description content="Teleported.in description"><meta name=author content="Anand Saha"><link rel=canonical href=https://teleported.in/categories/papers/><meta name=google-site-verification content="G-N2EQGH7WGQ"><link crossorigin=anonymous href=/assets/css/stylesheet.aeec76e59c7b349e53e4ea4176e875fb8ba0f24217ff7cccad34cc5462f887c1.css integrity="sha256-rux25Zx7NJ5T5OpBduh1+4ug8kIX/3zMrTTMVGL4h8E=" rel="preload stylesheet" as=style><link rel=icon href=https://teleported.in/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://teleported.in/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://teleported.in/favicon-32x32.png><link rel=apple-touch-icon href=https://teleported.in/apple-touch-icon.png><link rel=mask-icon href=https://teleported.in/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://teleported.in/categories/papers/index.xml title=rss><link rel=alternate hreflang=en href=https://teleported.in/categories/papers/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-N2EQGH7WGQ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-N2EQGH7WGQ")}</script><meta property="og:url" content="https://teleported.in/categories/papers/"><meta property="og:site_name" content="Teleported.in"><meta property="og:title" content="Papers"><meta property="og:description" content="Teleported.in description"><meta property="og:locale" content="en-us"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="Papers"><meta name=twitter:description content="Teleported.in description"></head><body class=list id=top><header class=header><nav class=nav><div class=logo><a href=https://teleported.in/ accesskey=h title="Teleported.in (Alt + H)"><img src=https://teleported.in/teleported-v2.png alt aria-label=logo height=50>Teleported.in</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://teleported.in/about/ title=about><span>about</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://teleported.in/>Home</a>&nbsp;»&nbsp;<a href=https://teleported.in/categories/>Categories</a></div><h1>Papers
<a href=/categories/papers/index.xml title=RSS aria-label=RSS><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" height="23"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></h1></header><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Landmark LLM Papers</h2></header><div class=entry-content><p>Introduction A list of curated landmark papers in the field of LLMs.
Foundational Efficient Estimation of Word Representations in Vector Space (Word2Vec) (2013) GloVe: Global Vectors for Word Representation (2014) Neural Machine Translation by Jointly Learning to Align and Translate (2014) Introduced the concept of attention Transformer Attention Is All You Need (2017) Introduced the Transformer architecture Self-Attention with Relative Position Representations (2018) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018) Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation (2021) RoFormer: Enhanced Transformer with Rotary Position Embedding (2021) Large Language Models Universal Language Model Fine-tuning for Text Classification (ULMFiT)(2018) Improving Language Understanding by Generative Pre-Training (GPT-1)(2018) Language Models are Unsupervised Multitask Learners (GPT-2)(2019) Language Models are Few-Shot Learners (GPT-3) (2020) What Can Transformers Learn In-Context? A Case Study of Simple Function Classes (2021) GPT-4 Technical Report (2023) Alignment Deep reinforcement learning from human preferences (2017) Training language models to follow instructions with human feedback (2022) Constitutional AI: Harmlessness from AI Feedback (2022) Scaling Laws, Emergence Scaling Laws for Neural Language Models (2020) Training Compute-Optimal Large Language Models (2022) Emergent Abilities of Large Language Models (2022) Prompt / Context Engineering Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (2021) Efficient Transformers Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (2019) Reformer: The Efficient Transformer (2019) Longformer: The Long-Document Transformer (2020) Generating Long Sequences with Sparse Transformers (2020) Big Bird: Transformers for Longer Sequences (2020) FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (2022) Survey Papers A Survey of Transformers (2022) Efficient Transformers: A Survey (2020) A Survey of Large Language Models (2023) On the Opportunities and Risks of Foundation Models (2022) Pre-train, Prompt, and Predict: A Survey of Prompting Methods in NLP (2021) Speed Always Wins: A Survey on Efficient Architectures for Large Language Models (2025)</p></div><footer class=entry-footer><span title='2025-12-20 23:27:27 -0400 -0400'>December 20, 2025</span>&nbsp;·&nbsp;<span>301 words</span>&nbsp;·&nbsp;<span>Anand Saha</span></footer><a class=entry-link aria-label="post link to Landmark LLM Papers" href=https://teleported.in/blog/2025/12/landmark-llm-papers/></a></article></main><footer class=footer><span>© 2025 Anand Saha.</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script></body></html>