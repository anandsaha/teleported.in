<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Network In Network architecture: The beginning of Inception | Teleported.in</title><meta name=keywords content="deep learning,architecture,inception"><meta name=description content="Introduction
In this post, I explain the Network In Network  paper by Min Lin, Qiang Chen, Shuicheng Yan (2013). This paper was quite influential in that it had a new take on convolutional filter design, which inspired the Inception line of deep architectures from Google.
Motivation
Anyone getting introduced to convolutional networks first come across this familiar arrangement of neurons designed by Yann LeCun decades ago:

Fig. LeNet-5
Yann LeCun&rsquo;s work (1989, 1998) triggered the convolutional approach, which takes into account the inherent structure of the incoming data (mostly image data) while propagating them through the network and learning about them."><meta name=author content="Anand Saha"><link rel=canonical href=https://teleported.in/blog/2017/10/network-in-network-architecture-the-beginning-of-inception/><meta name=google-site-verification content="G-N2EQGH7WGQ"><link crossorigin=anonymous href=/assets/css/stylesheet.98707a4793cd246eeb446b8c1cd2df61b84e032140b79bcbcf1fe7f830b7340b.css integrity="sha256-mHB6R5PNJG7rRGuMHNLfYbhOAyFAt5vLzx/n+DC3NAs=" rel="preload stylesheet" as=style><link rel=icon href=https://teleported.in/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://teleported.in/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://teleported.in/favicon-32x32.png><link rel=apple-touch-icon href=https://teleported.in/apple-touch-icon.png><link rel=mask-icon href=https://teleported.in/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://teleported.in/blog/2017/10/network-in-network-architecture-the-beginning-of-inception/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-N2EQGH7WGQ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-N2EQGH7WGQ")}</script><meta property="og:url" content="https://teleported.in/blog/2017/10/network-in-network-architecture-the-beginning-of-inception/"><meta property="og:site_name" content="Teleported.in"><meta property="og:title" content="Network In Network architecture: The beginning of Inception"><meta property="og:description" content="Introduction In this post, I explain the Network In Network paper by Min Lin, Qiang Chen, Shuicheng Yan (2013). This paper was quite influential in that it had a new take on convolutional filter design, which inspired the Inception line of deep architectures from Google.
Motivation Anyone getting introduced to convolutional networks first come across this familiar arrangement of neurons designed by Yann LeCun decades ago:
Fig. LeNet-5
Yann LeCun’s work (1989, 1998) triggered the convolutional approach, which takes into account the inherent structure of the incoming data (mostly image data) while propagating them through the network and learning about them."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2017-10-20T23:27:27-04:00"><meta property="article:modified_time" content="2017-10-20T23:27:27-04:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Architecture"><meta property="article:tag" content="Inception"><meta name=twitter:card content="summary"><meta name=twitter:title content="Network In Network architecture: The beginning of Inception"><meta name=twitter:description content="Introduction
In this post, I explain the Network In Network  paper by Min Lin, Qiang Chen, Shuicheng Yan (2013). This paper was quite influential in that it had a new take on convolutional filter design, which inspired the Inception line of deep architectures from Google.
Motivation
Anyone getting introduced to convolutional networks first come across this familiar arrangement of neurons designed by Yann LeCun decades ago:

Fig. LeNet-5
Yann LeCun&rsquo;s work (1989, 1998) triggered the convolutional approach, which takes into account the inherent structure of the incoming data (mostly image data) while propagating them through the network and learning about them."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://teleported.in/posts/"},{"@type":"ListItem","position":2,"name":"Network In Network architecture: The beginning of Inception","item":"https://teleported.in/blog/2017/10/network-in-network-architecture-the-beginning-of-inception/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Network In Network architecture: The beginning of Inception","name":"Network In Network architecture: The beginning of Inception","description":"Introduction In this post, I explain the Network In Network paper by Min Lin, Qiang Chen, Shuicheng Yan (2013). This paper was quite influential in that it had a new take on convolutional filter design, which inspired the Inception line of deep architectures from Google.\nMotivation Anyone getting introduced to convolutional networks first come across this familiar arrangement of neurons designed by Yann LeCun decades ago:\nFig. LeNet-5\nYann LeCun\u0026rsquo;s work (1989, 1998) triggered the convolutional approach, which takes into account the inherent structure of the incoming data (mostly image data) while propagating them through the network and learning about them.\n","keywords":["deep learning","architecture","inception"],"articleBody":"Introduction In this post, I explain the Network In Network paper by Min Lin, Qiang Chen, Shuicheng Yan (2013). This paper was quite influential in that it had a new take on convolutional filter design, which inspired the Inception line of deep architectures from Google.\nMotivation Anyone getting introduced to convolutional networks first come across this familiar arrangement of neurons designed by Yann LeCun decades ago:\nFig. LeNet-5\nYann LeCun’s work (1989, 1998) triggered the convolutional approach, which takes into account the inherent structure of the incoming data (mostly image data) while propagating them through the network and learning about them.\nIf you need a CNN refresher, this is an excellent read.\nThe idea of convolution is very simple and a genius one. Images have spatial information (height, width, and channels) and this arrangement of pixels is important to understanding their contents. Conventional neural networks would flatten their input before applying the weights, thereby losing the spatial information.\nFig. The typical arrangement of various layers in a CovNet (Img Credit: cs231n)\nConvolutional networks, on the other hand, operate directly on the images as is. The filters (also called kernels) are moved across the image left to right, top to bottom as if scanning the image and weighted sum of products are calculated between the filter and subset of the image the filter is superimposing on. This is the convolution operation. What is to be noted here is that the operation is linear. Of course these are then passed through various other operations like non-linear activations and pooling.\nFig: Conventional Convolution operation, and an idea (Img Credit: ResearchGate with my annotations)\nThe aspect to note in the convolution operation is the linearity of the operation. Does it need to be linear? Can it be something else that can extract richer features?\nThe idea of Network In Network This paper had a new take on how the convolution filters are designed and how we map extracted features to class scores. This formed the basis of the Inception architecture. Two new concepts were introduced in this CNN architecture design:\nMLPconv: Replaced linear filters with nonlinear Multi Linear Perceptrons to extract better features within the receipt field (see the figure above). This helped in better abstraction and accuracy. Global Average Pooling: Got rid of the fully connected layers at the end thereby reducing parameters and complexity. This was replaced by the creation of as many activation maps in the last layer as there are classes. This was followed by averaging these maps to arrive at final scores, which is passed to softmax. This is performant and more intuitive. MLPConv Traditional CNN architectures use linear filters to do the convolution and extract features out of images. The early layers try to extract primitive features like lines, edges, and corners, while the later layers build on early layers and extract higher-level features like eyes, ears, nose etc. These are called latent features.\nFig.Hierarchy of features extracted from various layers (Img Credit: https://arxiv.org/abs/1311.2901)\nNow, there can be variations in each of those features - there can be many different variations in eyes alone for e.g.. A linear filter (for e.g. to detect eyes) tries to draw straight lines to extract these features. Thus conventional CNN implicitly makes the assumption that the latent concepts are linearly separable. But a straight line may not always fit. The separation of the various types of eye features and non-eye features may not be a straight line. Using a richer nonlinear function approximator can serve as a better feature extractor.\nFig. Conventional linear convolution layer (Img Source: https://arxiv.org/abs/1312.4400 )\nThis paper introduced the concept of having a neural network itself in place of a convolution filter. The input to this mini network would be the convolution, and the output would be the value of a neuron in the activation. Hence it does not alter the input/output characteristics of traditional filters. This mini network, called MLPconv, can then convolved over the input. The benefit of having such an arrangement is two-fold:\nIt is compatible with the backpropagation logic of neural nets, thus this fits well into existing architectures of CNN’s It can itself be a deep model leading to rich separation between latent features Fig. MLPconv layer (Img Source: https://arxiv.org/abs/1312.4400 )\nGlobal Average Pooling In traditional CNN architectures, the feature maps of the last convolution layer are flattened and passed on to one or more fully connected layers, which are then passed on to softmax logistics layer for spitting out class probabilities. The issue with this approach is that it is hard to decode how the usual fully connected layers seen at the end of CNN architectures map to class probabilities. They are black boxes between the convolution layers and the classifier. They are also prone to overfitting and come with lots of parameters to train. An estimate says that the last FC layers contain 90% of the parameters of the network.\nCan we do better?\nFig. Global Average Pooling (Img Source: https://arxiv.org/abs/1312.4400 )\nIn the approach proposed by the paper, the last MLPconv layer produces as many activation maps as the number of classes being predicted. Then, each map is averaged giving rise to the raw scores of the classes. These are then fed to a SoftMax layer to produce the probabilities, totally making FC layers redundant.\nThe advantages of this approach are:\nThe mapping between the extracted features and the class scores is more intuitive and direct. The feature can be treated as category confidence. An implicit advantage is that there are no new parameters to train (unlike the FC layers), leading to less overfitting. Global average pooling sums out the spatial information, thus it is more robust to spatial translations of the input. Conclusion The paper demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets but more importantly gave a new direction to the design of convolution filters.\nReferences:\nhttps://arxiv.org/abs/1312.4400 https://openreview.net/forum?id=ylE6yojDR5yqX Please leave a comment below if anything was unclear or can be improved in the post.\n","wordCount":"1007","inLanguage":"en","datePublished":"2017-10-20T23:27:27-04:00","dateModified":"2017-10-20T23:27:27-04:00","author":{"@type":"Person","name":"Anand Saha"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://teleported.in/blog/2017/10/network-in-network-architecture-the-beginning-of-inception/"},"publisher":{"@type":"Organization","name":"Teleported.in","logo":{"@type":"ImageObject","url":"https://teleported.in/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://teleported.in/ accesskey=h title="Teleported.in (Alt + H)"><img src=https://teleported.in/teleported-v2.png alt aria-label=logo height=50>Teleported.in</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://teleported.in/about/ title=about><span>about</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://teleported.in/>Home</a>&nbsp;»&nbsp;<a href=https://teleported.in/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Network In Network architecture: The beginning of Inception</h1><div class=post-meta><span title='2017-10-20 23:27:27 -0400 -0400'>October 20, 2017</span>&nbsp;·&nbsp;1007 words&nbsp;·&nbsp;Anand Saha</div></header><div class=post-content><h3 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h3><p>In this post, I explain the <a href=https://arxiv.org/abs/1312.4400><code>Network In Network</code></a> paper by Min Lin, Qiang Chen, Shuicheng Yan (2013). This paper was quite influential in that it had a new take on convolutional filter design, which inspired the Inception line of deep architectures from Google.</p><h3 id=motivation>Motivation<a hidden class=anchor aria-hidden=true href=#motivation>#</a></h3><p>Anyone getting introduced to convolutional networks first come across this familiar arrangement of neurons designed by Yann LeCun decades ago:</p><p><img alt=LeNet loading=lazy src=/post_images/000-LeNet.png>
<em>Fig. <a href=http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf>LeNet-5</a></em></p><p>Yann LeCun&rsquo;s work (<a href=http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf>1989</a>, <a href=http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf>1998</a>) triggered the convolutional approach, which takes into account the inherent structure of the incoming data (mostly image data) while propagating them through the network and learning about them.</p><p>If you need a CNN refresher, <a href=http://cs231n.github.io/convolutional-networks/>this</a> is an excellent read.</p><p>The idea of convolution is very simple and a genius one. Images have spatial information (height, width, and channels) and this arrangement of pixels is important to understanding their contents. Conventional neural networks would flatten their input before applying the weights, thereby losing the spatial information.</p><p><img alt="Typical arrangement in covnet" loading=lazy src=/post_images/000-convnet.jpeg>
<em>Fig. The typical arrangement of various layers in a CovNet (Img Credit: <a href=http://cs231n.github.io/convolutional-networks/>cs231n</a>)</em></p><p>Convolutional networks, on the other hand, operate directly on the images as is. The filters (also called kernels) are moved across the image left to right, top to bottom as if scanning the image and weighted sum of products are calculated between the filter and subset of the image the filter is superimposing on. This is the convolution operation. What is to be noted here is that the operation is <em>linear</em>. Of course these are then passed through various other operations like non-linear activations and pooling.</p><p><img alt=convolution loading=lazy src=/post_images/000-convolution.png>
<em>Fig: Conventional Convolution operation, and an idea (Img Credit: <a href=https://www.researchgate.net/figure/309487032%5ffig2%5fFigure-2-a-Illustration-of-the-operation-principle-of-the-convolution-kernel>ResearchGate</a> with my annotations)</em></p><p>The aspect to note in the convolution operation is the <em>linearity</em> of the operation. Does it need to be linear? Can it be something else that can extract richer features?</p><h3 id=the-idea-of-network-in-network>The idea of Network In Network<a hidden class=anchor aria-hidden=true href=#the-idea-of-network-in-network>#</a></h3><p>This paper had a new take on how the convolution filters are designed and how we map extracted features to class scores. This formed the basis of the Inception architecture. Two new concepts were introduced in this CNN architecture design:</p><ul><li><strong>MLPconv</strong>: Replaced linear filters with nonlinear <code>Multi Linear Perceptrons</code> to extract better features within the receipt field (see the figure above). This helped in better abstraction and accuracy.</li><li><strong>Global Average Pooling</strong>: Got rid of the fully connected layers at the end thereby reducing parameters and complexity. This was replaced by the creation of as many activation maps in the last layer as there are classes. This was followed by averaging these maps to arrive at final scores, which is passed to softmax. This is performant and more intuitive.</li></ul><h3 id=mlpconv>MLPConv<a hidden class=anchor aria-hidden=true href=#mlpconv>#</a></h3><p>Traditional CNN architectures use linear filters to do the convolution and extract features out of images. The early layers try to extract primitive features like lines, edges, and corners, while the later layers build on early layers and extract higher-level features like eyes, ears, nose etc. These are called latent features.</p><p><img alt="Feature extraction" loading=lazy src=/post_images/000-zeiler-fertus.jpg>
<em>Fig.Hierarchy of features extracted from various layers (Img Credit: <a href=https://arxiv.org/abs/1311.2901>https://arxiv.org/abs/1311.2901</a>)</em></p><p>Now, there can be variations in each of those features - there can be many different variations in eyes alone for e.g.. A linear filter (for e.g. to detect eyes) tries to draw straight lines to extract these features. Thus conventional CNN implicitly makes the assumption that the latent concepts are linearly separable. But a straight line may not always fit. The separation of the various types of eye features and non-eye features may not be a straight line. Using a richer nonlinear function approximator can serve as a better feature extractor.</p><p><img alt=MLPConv loading=lazy src=/post_images/000-Network_In_Network_img1.png>
<em>Fig. Conventional linear convolution layer (Img Source: <a href=https://arxiv.org/abs/1312.4400>https://arxiv.org/abs/1312.4400</a> )</em></p><p>This paper introduced the concept of having a neural network itself in place of a convolution filter. The input to this mini network would be the convolution, and the output would be the value of a neuron in the activation. Hence it does not alter the input/output characteristics of traditional filters. This mini network, called MLPconv, can then convolved over the input. The benefit of having such an arrangement is two-fold:</p><ul><li>It is compatible with the backpropagation logic of neural nets, thus this fits well into existing architectures of CNN&rsquo;s</li><li>It can itself be a deep model leading to rich separation between latent features</li></ul><p><img alt=MLPConv loading=lazy src=/post_images/000-Network_In_Network_img2.png>
<em>Fig. MLPconv layer (Img Source: <a href=https://arxiv.org/abs/1312.4400>https://arxiv.org/abs/1312.4400</a> )</em></p><h3 id=global-average-pooling>Global Average Pooling<a hidden class=anchor aria-hidden=true href=#global-average-pooling>#</a></h3><p>In traditional CNN architectures, the feature maps of the last convolution layer are flattened and passed on to one or more fully connected layers, which are then passed on to softmax logistics layer for spitting out class probabilities. The issue with this approach is that it is hard to decode how the usual fully connected layers seen at the end of CNN architectures map to class probabilities. They are black boxes between the convolution layers and the classifier. They are also prone to overfitting and come with lots of parameters to train. An estimate says that the last FC layers contain 90% of the parameters of the network.</p><p>Can we do better?</p><p><img alt=MLPConv loading=lazy src=/post_images/000-Network_In_Network_img3.png>
<em>Fig. Global Average Pooling (Img Source: <a href=https://arxiv.org/abs/1312.4400>https://arxiv.org/abs/1312.4400</a> )</em></p><p>In the approach proposed by the paper, the last MLPconv layer produces as many activation maps as the number of classes being predicted. Then, each map is averaged giving rise to the raw scores of the classes. These are then fed to a SoftMax layer to produce the probabilities, totally making FC layers redundant.</p><p>The advantages of this approach are:</p><ul><li>The mapping between the extracted features and the class scores is more intuitive and direct. The feature can be treated as category confidence.</li><li>An implicit advantage is that there are no new parameters to train (unlike the FC layers), leading to less overfitting.</li><li>Global average pooling sums out the spatial information, thus it is more robust to spatial translations of the input.</li></ul><h3 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h3><p>The paper <code>demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets</code> but more importantly gave a new direction to the design of convolution filters.</p><p>References:</p><ul><li><a href=https://arxiv.org/abs/1312.4400>https://arxiv.org/abs/1312.4400</a></li><li><a href="https://openreview.net/forum?id=ylE6yojDR5yqX">https://openreview.net/forum?id=ylE6yojDR5yqX</a></li></ul><p><code>Please leave a comment below if anything was unclear or can be improved in the post.</code></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://teleported.in/tags/deep-learning/>Deep Learning</a></li><li><a href=https://teleported.in/tags/architecture/>Architecture</a></li><li><a href=https://teleported.in/tags/inception/>Inception</a></li></ul><nav class=paginav><a class=prev href=https://teleported.in/blog/2017/11/decoding-the-resnet-architecture/><span class=title>« Prev</span><br><span>Decoding the ResNet architecture</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Network In Network architecture: The beginning of Inception on x" href="https://x.com/intent/tweet/?text=Network%20In%20Network%20architecture%3a%20The%20beginning%20of%20Inception&amp;url=https%3a%2f%2fteleported.in%2fblog%2f2017%2f10%2fnetwork-in-network-architecture-the-beginning-of-inception%2f&amp;hashtags=deeplearning%2carchitecture%2cinception"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Network In Network architecture: The beginning of Inception on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fteleported.in%2fblog%2f2017%2f10%2fnetwork-in-network-architecture-the-beginning-of-inception%2f&amp;title=Network%20In%20Network%20architecture%3a%20The%20beginning%20of%20Inception&amp;summary=Network%20In%20Network%20architecture%3a%20The%20beginning%20of%20Inception&amp;source=https%3a%2f%2fteleported.in%2fblog%2f2017%2f10%2fnetwork-in-network-architecture-the-beginning-of-inception%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Network In Network architecture: The beginning of Inception on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fteleported.in%2fblog%2f2017%2f10%2fnetwork-in-network-architecture-the-beginning-of-inception%2f&title=Network%20In%20Network%20architecture%3a%20The%20beginning%20of%20Inception"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Network In Network architecture: The beginning of Inception on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fteleported.in%2fblog%2f2017%2f10%2fnetwork-in-network-architecture-the-beginning-of-inception%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Network In Network architecture: The beginning of Inception on whatsapp" href="https://api.whatsapp.com/send?text=Network%20In%20Network%20architecture%3a%20The%20beginning%20of%20Inception%20-%20https%3a%2f%2fteleported.in%2fblog%2f2017%2f10%2fnetwork-in-network-architecture-the-beginning-of-inception%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Network In Network architecture: The beginning of Inception on telegram" href="https://telegram.me/share/url?text=Network%20In%20Network%20architecture%3a%20The%20beginning%20of%20Inception&amp;url=https%3a%2f%2fteleported.in%2fblog%2f2017%2f10%2fnetwork-in-network-architecture-the-beginning-of-inception%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Network In Network architecture: The beginning of Inception on ycombinator" href="https://news.ycombinator.com/submitlink?t=Network%20In%20Network%20architecture%3a%20The%20beginning%20of%20Inception&u=https%3a%2f%2fteleported.in%2fblog%2f2017%2f10%2fnetwork-in-network-architecture-the-beginning-of-inception%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>© 2025 Anand Saha.</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>